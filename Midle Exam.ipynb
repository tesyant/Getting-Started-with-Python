{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it's your turn.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KTnya = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', \"'s\", 'a', 'sent', 'tokenize', 'test', '.', 'this', 'is', 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', \"'s\", 'your', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "print(KTnya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', \"'s\", 'a', 'sent', 'tokenize', 'test', '.', 'this', 'is', 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'now', 'it', \"'s\", 'your', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "LKata = [x.lower() for x in KTnya]\n",
    "print(LKata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NKata = len(LKata)\n",
    "NKata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "JKata = Counter(LKata).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sent', 4),\n",
       " ('this', 3),\n",
       " ('.', 3),\n",
       " ('is', 3),\n",
       " (\"'s\", 2),\n",
       " ('a', 1),\n",
       " ('tokenize', 1),\n",
       " ('test', 1),\n",
       " ('two', 1),\n",
       " ('three', 1),\n",
       " ('?', 1),\n",
       " ('4', 1),\n",
       " ('cool', 1),\n",
       " ('!', 1),\n",
       " ('now', 1),\n",
       " ('it', 1),\n",
       " ('your', 1),\n",
       " ('turn', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JKata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsbobot = []\n",
    "for kt, jm in JKata:\n",
    "    jbbt = jm/NKata\n",
    "    lsbobot.append((kt, jbbt))\n",
    "    lsbobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sent', 0.14285714285714285),\n",
       " ('this', 0.10714285714285714),\n",
       " ('.', 0.10714285714285714),\n",
       " ('is', 0.10714285714285714),\n",
       " (\"'s\", 0.07142857142857142),\n",
       " ('a', 0.03571428571428571),\n",
       " ('tokenize', 0.03571428571428571),\n",
       " ('test', 0.03571428571428571),\n",
       " ('two', 0.03571428571428571),\n",
       " ('three', 0.03571428571428571),\n",
       " ('?', 0.03571428571428571),\n",
       " ('4', 0.03571428571428571),\n",
       " ('cool', 0.03571428571428571),\n",
       " ('!', 0.03571428571428571),\n",
       " ('now', 0.03571428571428571),\n",
       " ('it', 0.03571428571428571),\n",
       " ('your', 0.03571428571428571),\n",
       " ('turn', 0.03571428571428571)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsbobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"This's a sent tokenize test.\",\n",
       " 'this is sent two.',\n",
       " 'is this sent three?',\n",
       " 'sent 4 is cool!',\n",
       " \"Now it's your turn.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PKalimat = sent_tokenize(text)\n",
    "PKalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', \"'s\", 'a', 'sent', 'tokenize', 'test', '.']\n",
      "['this', 'is', 'sent', 'two', '.']\n",
      "['is', 'this', 'sent', 'three', '?']\n",
      "['sent', '4', 'is', 'cool', '!']\n",
      "['Now', 'it', \"'s\", 'your', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in PKalimat:\n",
    "    sentences = nltk.word_tokenize(sentence)\n",
    "    print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'a', 'sent', 'tokenize', 'test']\n",
      "['this', 'is', 'sent', 'two']\n",
      "['is', 'this', 'sent', 'three']\n",
      "['sent', 'is', 'cool']\n",
      "['now', 'it', 'your', 'turn']\n"
     ]
    }
   ],
   "source": [
    "for sentence in PKalimat :\n",
    "    sentences = nltk.word_tokenize(sentence)\n",
    "    lowerAllSent = [katanya.lower() for katanya in sentences if katanya.isalpha()]\n",
    "    print(lowerSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'a', 'sent', 'tokenize', 'test']\n",
      "sent\n",
      "0.14285714285714285\n",
      "this\n",
      "0.10714285714285714\n",
      ".\n",
      "0.10714285714285714\n",
      "is\n",
      "0.10714285714285714\n",
      "'s\n",
      "0.07142857142857142\n",
      "a\n",
      "0.03571428571428571\n",
      "tokenize\n",
      "0.03571428571428571\n",
      "test\n",
      "0.03571428571428571\n",
      "two\n",
      "0.03571428571428571\n",
      "three\n",
      "0.03571428571428571\n",
      "?\n",
      "0.03571428571428571\n",
      "4\n",
      "0.03571428571428571\n",
      "cool\n",
      "0.03571428571428571\n",
      "!\n",
      "0.03571428571428571\n",
      "now\n",
      "0.03571428571428571\n",
      "it\n",
      "0.03571428571428571\n",
      "your\n",
      "0.03571428571428571\n",
      "turn\n",
      "0.03571428571428571\n",
      "['this', 'is', 'sent', 'two']\n",
      "sent\n",
      "0.14285714285714285\n",
      "this\n",
      "0.10714285714285714\n",
      ".\n",
      "0.10714285714285714\n",
      "is\n",
      "0.10714285714285714\n",
      "'s\n",
      "0.07142857142857142\n",
      "a\n",
      "0.03571428571428571\n",
      "tokenize\n",
      "0.03571428571428571\n",
      "test\n",
      "0.03571428571428571\n",
      "two\n",
      "0.03571428571428571\n",
      "three\n",
      "0.03571428571428571\n",
      "?\n",
      "0.03571428571428571\n",
      "4\n",
      "0.03571428571428571\n",
      "cool\n",
      "0.03571428571428571\n",
      "!\n",
      "0.03571428571428571\n",
      "now\n",
      "0.03571428571428571\n",
      "it\n",
      "0.03571428571428571\n",
      "your\n",
      "0.03571428571428571\n",
      "turn\n",
      "0.03571428571428571\n",
      "['is', 'this', 'sent', 'three']\n",
      "sent\n",
      "0.14285714285714285\n",
      "this\n",
      "0.10714285714285714\n",
      ".\n",
      "0.10714285714285714\n",
      "is\n",
      "0.10714285714285714\n",
      "'s\n",
      "0.07142857142857142\n",
      "a\n",
      "0.03571428571428571\n",
      "tokenize\n",
      "0.03571428571428571\n",
      "test\n",
      "0.03571428571428571\n",
      "two\n",
      "0.03571428571428571\n",
      "three\n",
      "0.03571428571428571\n",
      "?\n",
      "0.03571428571428571\n",
      "4\n",
      "0.03571428571428571\n",
      "cool\n",
      "0.03571428571428571\n",
      "!\n",
      "0.03571428571428571\n",
      "now\n",
      "0.03571428571428571\n",
      "it\n",
      "0.03571428571428571\n",
      "your\n",
      "0.03571428571428571\n",
      "turn\n",
      "0.03571428571428571\n",
      "['sent', 'is', 'cool']\n",
      "sent\n",
      "0.14285714285714285\n",
      "this\n",
      "0.10714285714285714\n",
      ".\n",
      "0.10714285714285714\n",
      "is\n",
      "0.10714285714285714\n",
      "'s\n",
      "0.07142857142857142\n",
      "a\n",
      "0.03571428571428571\n",
      "tokenize\n",
      "0.03571428571428571\n",
      "test\n",
      "0.03571428571428571\n",
      "two\n",
      "0.03571428571428571\n",
      "three\n",
      "0.03571428571428571\n",
      "?\n",
      "0.03571428571428571\n",
      "4\n",
      "0.03571428571428571\n",
      "cool\n",
      "0.03571428571428571\n",
      "!\n",
      "0.03571428571428571\n",
      "now\n",
      "0.03571428571428571\n",
      "it\n",
      "0.03571428571428571\n",
      "your\n",
      "0.03571428571428571\n",
      "turn\n",
      "0.03571428571428571\n",
      "['now', 'it', 'your', 'turn']\n",
      "sent\n",
      "0.14285714285714285\n",
      "this\n",
      "0.10714285714285714\n",
      ".\n",
      "0.10714285714285714\n",
      "is\n",
      "0.10714285714285714\n",
      "'s\n",
      "0.07142857142857142\n",
      "a\n",
      "0.03571428571428571\n",
      "tokenize\n",
      "0.03571428571428571\n",
      "test\n",
      "0.03571428571428571\n",
      "two\n",
      "0.03571428571428571\n",
      "three\n",
      "0.03571428571428571\n",
      "?\n",
      "0.03571428571428571\n",
      "4\n",
      "0.03571428571428571\n",
      "cool\n",
      "0.03571428571428571\n",
      "!\n",
      "0.03571428571428571\n",
      "now\n",
      "0.03571428571428571\n",
      "it\n",
      "0.03571428571428571\n",
      "your\n",
      "0.03571428571428571\n",
      "turn\n",
      "0.03571428571428571\n"
     ]
    }
   ],
   "source": [
    "for sentence in PKalimat :\n",
    "    sentences = nltk.word_tokenize(sentence)\n",
    "    lowerSent = [everyWord.lower() for everyWord in sentences if everyWord.isalpha()]\n",
    "    print(lowerSent)\n",
    "    \n",
    "##         value of the word\n",
    "#         print(valKata)\n",
    "#         print(val)\n",
    "    for valKata, val in lsbobot:\n",
    "        for shortKata in lowerSent:\n",
    "            if (valKata == shortKat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
